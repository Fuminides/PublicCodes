# -*- coding: utf-8 -*-
"""
Created on Wed Sep 12 15:47:44 2018

@author: Javier Fumanal Idocin

sooooooooossssssoooooooo+//:/:://++oooo+oooo++/////////////////+++ooooo+oooyyyyyssyyyyyysyyhyhyyyyys
so++ooo++oooosooosoosoo//:+++/+/+ossyyyyysysoo+++++/:::-----:://+++ooooooosyyhhhhys+ooooooooooossyso
o+/:--:/+oossssssssssso//:/+++++oyhhyyss+++/:////:--..--------::::://++ooossssyddhhhyyyso+//////+oo+
+.-::--:/+oosyssssosssssssoooosss+/////:.--:::------.----...----::://++++ooosssyyyso+o::::::///+++++
+//:::////+++++++++osyysso++ooo/::++yhdssyyyo/:---------..........-:::///++++++++++++o/-::::///////:
+/:-:////////////++oyyhyysoo++/-ohdNmmmmmNNmmmNmdyo/:---..............-:::///::::///:/:::::://::///:
+/:://///////////+oosssyhyso/:-oNNNNmhydNNmmmddmmNNNNms/-..--.---......-:://////////////:::-----://:
/:::::/:///////////+++ossoso/:-mNNNhyoohNNNNNNNmmdmNMMMNmho:--------:::://++++///++++++++//:://:////
::-:-:::://///////////++++++/:-dMNho+oohNNMNNNNNNmdddmNMMMNNho+///:://///+++++++++oo++++++//////////
/:---::---::/::-:///::////+++/:yMms++osmNNMNNNNNNNMNmdmNMMMMMNdo++////+++ooossyyss++++++++//::////++
/:-----:--::::-::::/::///+++/++oNmssyshmNNMNNNNNNNNNNNmmNmNNNMMNy+++++++ooooossso+/////////////::://
/::--::::::///:///:::::/:---:::-+mhdmNNNNNMMNNNNNmhdNNNmmmNNNNMMMmo/:////+oo+//:-..://:::::::::-:::-
/::::::::///////:::::::::::::::./y:+dmNNNNNNMNMMmyhhhmMMMNmmNNMMMMNs........```.-:://///::::::--::::
/:::://////:////:/::://:/+++/+/.h/:-+syhhhhhdmNMmdmddNMMMMNmdmNNMMMMd:...--.---://::::///:::::::::::
/:/::++//::::::///:///+++++/:-``d::--:::-//+//sdNNNNNMMMNMNNmmmNNNMMMNo...::::/::----.-:::::::::::::
..```::-----:://////+oo+//:..``-h//:..-/hdmdsoosydNNMMMMNNNNNNmNNNNNNNNy:..`.......````-:::::://::::
..``...---:::///::/++++//-.--..oNmmh/--odyhh/o+/:/shmNNMNNNNNNNNNNNNNNNNd.````....-----:::::::::::--
-....---::://////+o+//::-..`--.yNdh+/--/o/-:--...-:+ymNNNNNNNNNNNNNNNNNNNy.`..-----.-------::::::---
/:::::::::://+so////::--.......mNs:-:-:///......--:+symNNNNNNNNNNNNNNNNNd/``.....----...-.----:---..
/:::::::/:://+o/////:::--...`.:NNo----://:-....--:/+oshNNNNNNNNNNNNNNNNN/```````````.``.....----....
/::-:::::::/o+/////:::::--..../NNd::--+o++---.--/++++shNNNNNNNNNNNNNNNNNm/``````````````````........
::-:::::-::++:///++/:/:::-..--+NNNs:/ymdhs..---:/+//oshNNNNNNNNNNNNNNNmmmh``````````````````````````
--::::::://+///++o+//:::::::::/mNNm:+shdho:---:////+syhmNNNNNNNNNNmmNNs..`  ``````.`````...`....````
.-::::://++++/+o+o++//::::::---hNNNsdh+shhhs+//+++osyddmNmNNNNNNNNh:yms```````..::/-.....-.-::::::--
..--::://+o++oo+/+++/::::-.````/NNNmdo/o//++so+oosyddmmmNNNNNNNNNNNy...-......-:+/:----.--:::::::::-
..----://+s++++//+++//:/:-.````-NmNNh::o////+ooyhddmmmmmNNmNNNNNNNNNh::/::-::::/::////:-:shs:-----..
.`...-://+o+/++//+++///::-.````:mNNNm+-+/::/+oydmmmmmmmmNNNNNNNNNNNNNssssyhhmNddhssssyhydmmdhosoo//:
-.....++oo+///+///++///:::.````.hNNNmyooo+osshmNNNmmmmmmmNNNNNNNNNNNNhhdmNNNNNNNNmNNmdhhdmddddddhyyy
//:---+oh+/////////////:/:-.``..-mNNmdyyyyyhdmNNNmmmmmdddmNNNNNNNNMNNmhydmmNNNNNNNNNmddmNNNmmmdddddh
+///::++do+++//////+//////:-.---.hhdNNmmmmddddmmdddmdddhdmNNNNNNMNMNMMmy+yhddmdmmmmNmNmmNNNNNmdddddd
o+++//++ms+o//++++/++++/////::/-./+oNNhhd//syyhhddddhhhddmNNNNMMMMMMMNNo.:/osyyhhmdmmmmmNNmNNmmmmmmm
soo+o+oods++ooo+/++++:+////+///:-..:dm+yh::/+syhhhhhhhdddmNNNNNMMMMMMMmm/.-/+://oshdddmmNNmmNNmmmmmm
oo+//+sshy+oo++o:y++/:+///+++/:::-../dys::::/+ossyyyyhhhddmNmmNNNNNNMMNd+-:::::/+oyhdhdmNNNmmmmmdmmm
+++++oyhhhss+++++ohooy///////::/----:+/:-:::::/+ossssyyhhhhysyddhshhhdmmmho//+osyyddhhhdmmNmmmddhhdd
sssoohsddmdyooo+//hoo+::::/+:::/:o///:://:/-::-::::://////:::::::::::+:://+shdmmddyyysssssysssssyyyy
mNNNmmhmmmhyyyyyyyyyhhhhhs+.`````````.`...:::+/:+/://:/+:://++/:///+//+://://+++shdddhhhyyhhhhhhhhhy
NNmmNmmmmmmmmmmmmmmmmmddhs/::--.` `    `  ````` `. `.``````.-````.`..````.`..`.`.-:sdddddhddhhdddhhh
mmmmmmmmmNmmmmmmmmmddhyyoo++osso+o+//:-:--:.`` `.. ````.``  ````````..``.---/::::/+shmmmNmmmmmmmmmdd
NmmmmmmmNNNmNmmmmdhdmmmmmmmdhhhyoo/+++/////+++ooso+/oo+sooo+o+o+osososssyyyyyyyyyyyyhhhhdmmmmmmmmmmm
mmmmmmmmmmmmmmmmhhmdhNmdNNmNmddNNdddmdhhysssooyyso++++++++++/++/++sso+ooossssyyhhdddmmNNNNNNNNNmmmmm
mmmmmmmmmmmmmmmhhmmdddmmNNdmddmmmmmNNddmNdNmmhNmNmmmmmmmdmmmdmmmmmNNNmmhmNmmNNNNNMNNMMMMNMNNMMNNNNmm
NNNmmNNNNmNmmmddmNddmdmmmNdmdmmmmmmNmmmmmmNdmmNmNmNNmmNNhNmNdNmmNNNNNNNdmNNmmNNNNNNNMNNMNNNNNMNNNNNm
NNNNNNNNNNNNmddNNNmdmmmmmNddddmNdmmNmmmmmNNmmmNmMmNNmdmmhNmNdNmmNNmmNNNNmNmNmmmmNNNmMNNNNdNNNNNNNNNN
NNNNNNNNNNmmmhmdNmmmmmmmmNddmmmmmdmNdddmmNmNNmNmMmmNmdmmdNmNdNmdNmdmmNmNmNmNmmmmmNNmNNNNmdNNNNNNNNNN
NNmNNNNNNmdmmhmhNNmmmmmmmNmdmmdmddmNmddmNNmmmNmNMmmNmddddNmNdNmmNmdmmNmNNNmNmmmmmmNmNNmdymNNNNNNNNNN
NNmNNNNNmdmNmdmNNMNmmmmmmmmdmmddddmNmmmmNMmdmNdNNmmmddddmNmmdNmmmdddmmmNNNmNmmmmNNmmMNdohmNNNNNNNNNN
NNNNNNmmmdNmmdNmdmNNNNmmmmNdmNmdmmmNdddmNNmdmmmNNmmmmddmNmmNmNmmmmddmdmNNNmNmmmmNmdNNmdmmdNNNNNNNNNN
NNNNNmdNdNmmmdmmmhdNNMNmmmNmmNmmdmmNdddmNNmmmmNNNmmmmmmNNmmNmNmdmmdddhmNNNmNmmmNNmmMNdNNdmNMNNNNNNNN
NNNNNdmNdNdmNmmNNmddmNMNNNNNmNmmmmmmddmNNMNmNNNNNmNmNNNNmdmNmNNdmmdddhmNMNmmmmmNNmNMmmmhdNNNNNNNNNNN
NNNmddNNmNmdmNdmmmmmmmMNNNNNmNmmmmmNdmmNNMNmdhhdhyhdmNNmmmmmmNNddmhmmdmNMNmmmmNNmNMNmmmhmNMNNNNNNNNN
NNNdmNNmmmmmmNmmmmmddmNNNNNNmNmmmmNNdmmNNmo:..-/::-:+hmmmmmmmNNddmhmmmmNMNmmmmNNNNNmdddmNNNNNNNNNNNN
NNmmNNNNmmmmmmmNNNmdmNNmNNNNNNNNmNNmdddd+.`````./++oshmNmdmmmNNmdmmmmmmNMNmNNNNNNNdyhdmNNMNNNmNNmNNN
NmmNNNmmNNNNNNNNNNmmNNNNmNNNNNNNNNhssho````````./+:--+dNmmmdmNNmmmmNmmNNNNNNNNNmdhhddNNNNNNNNNNNNNNN
dmNNNmddmNNNNMMMMMNNNNNNNNNNNNmNNNo:oo.`````````-++//+hNmmmmNNNNmmmNNmNNNNNNNmddddmNNNmNNNNNNNNNNNNN
mNNNmdmmNNNNNNNNNNmmmNNMMMNNhyoMNh//:+.`````` `.-+/::ohNNNNmNNNNmmNmNNNMNNNmdhdmNNNmmmmNNNNNNNNmNmNN
mNNmmNNNNNNNNdddmdssdmmNNmdh+//dm:..`+/`````````-osossymNNNNNNNNmNNNNNMMNNddmmNNmdhddmmNNNNNNNNmmNNN
NNmmNNNNNNNmhhdmmmmmNddNddddh+++Ns-./ds:+``````-/syhdmNNNNNNNNNNNNNNNMNmmmmNmdyddddmmNNNNNmNNNNNNNNN
NmmNNNNNNNNmmNmNNMNNmmmmmmmmmysssms--.-:o:---/oydmNNNNNNNNNNNNNNNNNMNNmmmmmhhhhdmmNmNmmmmNNNNNNNNNNm
mddNMMNNNmmmmNNMMmNNNmNmmNdNmNhssomh+:o++hhhdmNNMMMNmNNNNNNNNMNNNMNNmmdhddmddddmNmdddmNNNNNNNNNNNNNm
mmmmNNNNmmmNNMMNmmNNNNmNmNdmNNNhsshdmddh+dmNNMMMMMMNddmNNMMMMMNmddhhyhhhddddmddhyhNNNNNNmNmmmNNNNNNm
dmNdhmmmmNNNNmNmmNNNmNmNNmmdNNNNmhhddmMMMNNNNMMMMMMMNdmmmNNmdmddmmmhhdmmmmdhhdmddmmmmmmmmmNNNNNNNNNm
mmNydmNNNNNNNmNdNNNNmNNmNNNmdNNNNNmmmmmNNNNNNNNNNNNNNmmddhdhyyddddmmmddhhdmmmNmmdhhydmmmmNmNNNNmmmmm
"""

from importlib import reload

import numpy as np
import pandas as pd
import holoviews as hv


import Compute_G as cg

import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram
from scipy import sparse

import affinity as af

import networkx as nx
from networkx.algorithms.community import greedy_modularity_communities, girvan_newman
import community
from modularitydensity.metrics import modularity_density

#########################################
# EXPERIMENTS PART: ONLY HERE FOR CONVENIENCE.
#   These functions reproduce the comparisons using networkx and iGraph.
########################################

def partition_to_array(partition):
    res = {}
    for ix, c in enumerate(partition):
        size = len(c)
        for jx, ja in enumerate(c):
            res[ja] = ix #.append(ix)

    return res

def numpy_array_to_igraph(array):
    import igraph as ig

    G = ig.Graph()
    rows,cols = array.shape
    weight = []
    for i in range(rows):
        G.add_vertex(i)

    for i in range(rows):
        for j in range(cols):
            if array[i, j] > 0:
                G.add_edge(i, j)
                weight.append(array[i,j])
    G.es['weight'] = weight
    return G

def compute_eval_metrics(G, array_connection, clustering, labels=None):
    if labels is None:
        return (nx.community.quality.modularity(G, get_partitions(G, clustering)), modularity_density(array_connection, clustering, np.unique(clustering)), len(np.unique(clustering)))
    else:
        from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score
        aux = []
        aux_c = []
        for key in G.node:
            aux.append(labels[key])
            aux_c.append(clustering[key])

        clustering = aux_c
        labels = aux
        labels = np.array(labels, dtype = int)

        return ((nx.community.quality.modularity(G, get_partitions(G, clustering)),
        modularity_density(array_connection, clustering, np.unique(clustering)), len(np.unique(clustering))) ,
                (adjusted_rand_score(labels, clustering),adjusted_mutual_info_score(labels, clustering)))

def eval_net_algs(adjacency_list, labels=None):
    import igraph as ig
    import collections

    if (labels is not None) and (not isinstance(labels, collections.Mapping)):
        aux = {}
        for i in range(len(labels)):
            aux[str(i)] = labels[i]

        labels = aux

    #Borgia Algorithm
    clustering_result, array_connection, dic, rdic, _ = calc_result_clustering(adjacency_list, file_name = 'borgia_result.pdf')
    G = nx.Graph(array_connection)
    G = nx.relabel_nodes(G, dict((v, k) for k, v in dic.items()))
    G2 = numpy_array_to_igraph(array_connection)

    for ix, elem in dic.items():
        dic[ix] = clustering_result[elem]

    #bans = get_partitions(G, clustering_result)
    if labels is None:
        b_results = compute_eval_metrics(G, array_connection, clustering_result, labels)
    else:
        b_results = compute_eval_metrics(G, array_connection, dic, labels)

    #Girvan-newman algorithm
    gans = girvan_newman(G)
    best = -np.Inf
    for i in gans:
        a = nx.community.quality.modularity(G, i)
        if a >= best:
            gn_sol = i
            best = a
    if labels is None:
        gn_results = compute_eval_metrics(G, array_connection, list(partition_to_array(gn_sol).values()), labels)
    else:
        gn_results = compute_eval_metrics(G, array_connection, partition_to_array(gn_sol), labels)

    #Newman algorithm
    nans = greedy_modularity_communities(G)
    if labels is None:
        greedy_results = compute_eval_metrics(G, array_connection,  list(partition_to_array(nans).values()), labels)
    else:
        greedy_results = compute_eval_metrics(G, array_connection, partition_to_array(nans), labels)

    #Leading eigenvector
    lans_aux = G2.community_leading_eigenvector(weights='weight').membership
    lans = {}
    for ix in range(len(lans_aux)):
        lans[list(G.node)[ix]] = lans_aux[ix]

    if labels is not None:
        eigen_vector = compute_eval_metrics(G, array_connection, lans, labels)
    else:
        eigen_vector = compute_eval_metrics(G, array_connection, list(lans.values()), labels)

    #Infomap
    info_ans_aux = G2.community_infomap(edge_weights='weight').membership
    info_ans = {}
    for ix in range(len(info_ans_aux)):
        info_ans[list(G.node)[ix]] = info_ans_aux[ix]

    if labels is not None:
        info_map_vector =  compute_eval_metrics(G, array_connection, info_ans, labels)
    else:
        info_map_vector = compute_eval_metrics(G, array_connection, list(info_ans.values()), labels)

    #Label communities
    aans =  G2.community_label_propagation(weights='weight').membership
    paans = {}
    for ix in range(len(aans)):
        paans[list(G.node)[ix]] = aans[ix]
    if labels is not None:
        flow_results =  compute_eval_metrics(G, array_connection,  paans, labels)
    else:
        flow_results = compute_eval_metrics(G, array_connection, list(paans.values()), labels)

    #Lovaine algorithm
    lovaina_array_com = community.best_partition(G)
    #lans = get_partitions(G, list(lovaina_array_com.values()))
    if labels is None:
        lovaina_results = compute_eval_metrics(G, array_connection, list(lovaina_array_com.values()), labels)
    else:
        lovaina_results = compute_eval_metrics(G, array_connection, lovaina_array_com, labels)

    return greedy_results, gn_results, flow_results, eigen_vector, info_map_vector, lovaina_results, b_results

########################################################################################################
#   BORGIA CLUSTERING
########################################################################################################

def color_simple_map(x, posibles):
    colores_paleta = ['blue', 'red', 'green', 'orange', 'purple', 'lightcoral', 'darkorange', 'olive']
    
    return colores_paleta[np.argmax(np.equal(posibles, x))%len(colores_paleta)]

def random_sample(N, survival_rate):
    random_mat = np.random.rand(N.shape[0])
    activos =  random_mat < survival_rate

    return N[activos]

def boxplot_times(time_results):
    aux = ['1','0.9','0.8','0.7','0.6','0.5','0.4','0.3', '0.2', '0.1']
    aux.reverse()
    time_results.columns = aux
    medias = np.mean(time_results)

    prueba = time_results.melt()
    
    box_times = hv.BoxWhisker(prueba, 'variable','value', label='Execution time for different number of edges')
    times_plot = hv.Curve((aux, medias)).opts(color='red', width=400)
    box_times  = box_times.redim.label(variable='Percentage of the total edges', value='Time (s)')
    times_plot = times_plot.redim.label(x='Percentaje of the total edges')
    
    return box_times * times_plot
    
    
# =============================================================================
# ~LOAD DATASETS -> Established examples.
# =============================================================================
def load_got():
    got = pd.read_csv('./edge_lists/got.csv', sep=',')
    got.drop(['Type', 'id'], axis=1, inplace=True)
    got.sort_values(['weight'], ascending=False, inplace=True)
    
    return got

def load_dolphins():
    path_dolphin = './edge_lists/dolphins.gml'
    nx_gml_gr = nx.read_gml(path_dolphin)
    dolphins = pd.DataFrame(nx_gml_gr.edges)
    dolphins.columns = ['Source', 'Target']
    #labels = [x[1]['value'] for x in nx_gml_gr.nodes(data=True)]
    labels_csv = pd.read_csv('./edge_lists/dolphins_labels.csv')
    nodes = nx_gml_gr.node
    labels = {}
    for node in nodes:
        labels[node] = labels_csv['modularity_class'][labels_csv['Label'] == node].values[0]

    return dolphins, labels

def load_polbooks():
    path_dolphin = './edge_lists/polbooks.gml'
    nx_gml_gr = nx.read_gml(path_dolphin)
    dolphins = pd.DataFrame(nx_gml_gr.edges)
    dolphins.columns = ['Source', 'Target']

    translator = {}
    translator['l'] = 0
    translator['c'] = 1
    translator['n'] = 2

    labels_t = [(x[0], x[1]['value']) for x in nx_gml_gr.nodes(data=True)]
    labels = {}
    for i in labels_t:
        labels[i[0]] = translator[i[1]]

    return dolphins, labels

def load_football():
    path_dolphin = './edge_lists/football.gml'
    nx_gml_gr = nx.read_gml(path_dolphin)
    dolphins = pd.DataFrame(nx_gml_gr.edges)
    dolphins.columns = ['Source', 'Target']

    labels_t = [(x[0], x[1]['value']) for x in nx_gml_gr.nodes(data=True)]
    labels = {}
    for i in labels_t:
        labels[i[0]] = i[1]

    return dolphins, labels

def load_zachary():
    zachary = pd.read_csv('./edge_lists/zachary.csv', sep=' ')
    zachary.columns = ['Source', 'Target']
    labels = np.zeros((34,))
    zachary_ground1 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 17, 18, 20, 22])-1
    labels[zachary_ground1] = 1
    labels_dict = {}
    for i in range(len(labels)):
        labels_dict[i+1] = labels[i]
    
    return zachary, labels_dict

def load_got_cut():
    control['sym'] = True
    got_pray = pd.read_csv('./edge_lists/got_35.csv', sep=',')
    
    return got_pray

def load_heart_of_darkness():
    control['sym'] = True
    heart = pd.read_csv('./edge_lists/heart_crop.csv', sep=';')
    
    return heart

# =============================================================================
# ~ EUROVISION! EEEEEEERES TU, COMO EL AGUA DE MI FUEEENTE, EEEERES TUUUU...
# =============================================================================
def load_euro_finals(judge = 'J', year = None):
    control['sym'] = False
    euro_raw = pd.read_csv('./edge_lists/eurovision.csv',sep=';')
    euro_dropped = euro_raw.drop(['Edition', 'Duplicate'], axis = 1)
    euro_finals = euro_dropped.loc[(euro_dropped['(semi-) final'] == 'f'),:].drop('(semi-) final', axis = 1)
    euro = euro_finals.loc[euro_finals['Jury or Televoting'] == judge,:].drop('Jury or Televoting', axis = 1)
    
    if year is None:
        euro = euro.drop('Year', axis=1)
        sum_puntos = euro.groupby(['From country','To country']).sum()
        res = pd.DataFrame(columns = ['Source', 'Target', 'Weight'])
        for ix, countries in enumerate(sum_puntos.index):
            aux = pd.DataFrame([[countries[0], countries[1], sum_puntos.iloc[ix, -1]]], columns =['Source', 'Target', 'Weight'])
            res = pd.concat([res, aux], ignore_index=True)
            
        return res
    elif type(year) == int:
        indexes_ano = euro.groupby('Year').groups[year].values
        euro = euro_raw.iloc[indexes_ano][['From country', 'To country', 'Points      ']]
        euro.columns = ['Source', 'Target', 'Weight']
        
        return euro
    else:
        indexes_anos = euro.groupby('Year')
        res = pd.DataFrame(columns=['Source', 'Target', 'Weight'])
        for yea in year:
            indexes_ano = indexes_anos.groups[yea].values
            euro = euro_raw.iloc[indexes_ano][['From country', 'To country', 'Points      ']]
            euro.columns = ['Source', 'Target', 'Weight']
            res = pd.concat([res, euro], ignore_index=True)
            
        grupos = res.groupby(['Source', 'Target']).aggregate(np.sum)
        res = pd.DataFrame(columns=['Source', 'Target', 'Weight'])
        for i in range(grupos.shape[0]):
            elemento = grupos.iloc[i]
            emisor, receptor = elemento.name
            peso = elemento[0]
            res = res.append({'Source':emisor,'Target':receptor,'Weight':peso}, ignore_index=True)
            
        return res.loc[res['Weight'] > 0,:]
        
    
# =============================================================================
# ~EVAL FUNCTIONS
# =============================================================================
def eval_zachary(clustering):
    zachary_ground1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 17, 18, 20, 22]
    zachary_ground2 = [10, 15, 16, 19, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]
    
    print(np.mean(clustering[np.array(zachary_ground1)-1]==np.unique(clustering)[0]))
    print(np.mean(clustering[np.array(zachary_ground2)-1]==np.unique(clustering)[1]))
    print(clustering[np.array(zachary_ground1)-1])
    print(clustering[np.array(zachary_ground2)-1])

    return (np.sum(clustering[np.array(zachary_ground1)-1]==np.unique(clustering)[0]) + np.sum(clustering[np.array(zachary_ground2)-1]==np.unique(clustering)[1])) / (len(zachary_ground1) + len(zachary_ground2))


# =============================================================================
# ~MAIN EXECUTION FUNCTIONS
# =============================================================================
def full_process_csv(actor_edges, symmetric = True, csr = False):
    '''
    actor_edges -> a loaded csv file with the edges in the format 'Source' 'Target' 'Weight'    
    '''
    entidades = np.unique(actor_edges[['Source', 'Target']])
    num_entidades = len(entidades)
    
    diccionario = {}
    diccionario_reverso = {}

    identificador = 0
    for i in range(0, len(entidades)):
        diccionario[entidades[i]] = identificador
        diccionario_reverso[identificador] = entidades[i]
        identificador += 1

    if not csr:
        A = np.zeros([num_entidades, num_entidades], np.float64)
    else:
        A = sparse.lil_matrix((num_entidades, num_entidades), dtype=np.float64)

    names = list(diccionario.keys())
    
    for i in range(0, actor_edges.shape[0]):
        source = actor_edges.iloc[i][0]
        target = actor_edges.iloc[i][1]
        try:
            peso = actor_edges.iloc[i][2]
        except IndexError:
            peso = 1
        A[diccionario[source], diccionario[target]] = peso

        if symmetric:
            A[diccionario[target], diccionario[source]] = peso #Comment this to load eurovision

    
    linkeos = []
    if not csr:
        alpha = control['alpha']
        data = af.connexion2affinity_important_friend(A)*alpha + af.connexion2affinity(A)*(1-alpha) + np.identity(A.shape[0])
        p2 = data.copy()
        p2 = pd.DataFrame(p2)
        p2.columns = names
        p2.index = names
    else:
        data = af.connexion2affinity_important_friend(A, csr) + sparse.eye(A.shape[0])
        p2 = None

    masses2 = np.sum(A>0,axis=1).flatten()
    
    di = diccionario.copy()
    for ix, i in enumerate(di.keys()):
        if masses2.shape[0] == 1:
            di[i] = (masses2[0, ix])
        else:
            di[i] = (masses2[ix])
        
    
    return data, A, p2, di, diccionario, diccionario_reverso, linkeos

def borgia_clustering(data, masses, control, dic=None, rdic=None, link=None, sparse_flag = False):
    delta = control['delta']
    epsilon = delta*2
    num_clusters = control['num_clusters']
    h = control['h']
    unitary = control['unitary']
    connect = control['connect']
    normalization = control['normalization']
    cl_test = control['cl_test']
    correccion_dim = ['correccion_dim']
    adaptativo = control['adaptativo']
    verbose = control['verbose']
    p = control['p']
    penalty = control['penalty']
    sparse = control['sparse']
    
    if dic is None:
        dic = {}
        rdic = {}
        for i in range(0, data.shape[0]):
            dic[i] = i
            rdic[i] = i

    result = cg.gravitational_clustering_numG(data, delta, epsilon, p, num_clusters, h, unitary, masses=masses, connect=connect,
            normalization=normalization, cl_test=cl_test, conexiones=data, correccion_dim=correccion_dim, adaptativo=adaptativo,
            penalty=penalty, verbose=verbose, sparse_flag = sparse_flag, diccionario=dic, diccionario_reverso=rdic)

    #if num_clusters > 0:
    #    result = cg.configuraciones[-num_clusters]

    #if verbose:
    #   print("NÂº clusters: ", len(np.unique(result)))
        
    return result

def plot_dendrogram(clustering, names, file_name = None):
    '''
    Requires the 'linkeos' global variable to be ready.
    Use with caution.
    '''
    #See the dendogram 
    vis, coleur = cg.process_basic_dendrogram(cg.linkeos, clustering)
    coleur = np.array(coleur)
    for ix, i in enumerate(np.unique(coleur)):
        coleur[coleur==i] = ix
    
    fig = plt.gcf()
    diccionario_grafico = dendrogram(np.double(vis), labels=names, orientation='right', leaf_font_size=10, link_color_func=lambda x: color_simple_map(coleur[x], np.unique(coleur))) #leaf_font_size=2,
    if not (file_name is None):
        fig.set_size_inches(21.5, 12.5)
        plt.savefig(file_name)
    
    return diccionario_grafico

def format_output_evals(evaluation_tuples, eval_metrics = ('Modularity', 'Modularity Density', 'Communities', 'ARI','NMI'), names = ('Greedy Modularity', 'Girvan-Newman', 'Label propagation', 'Leading eigenvector', 'Infomap', 'Lovaine', 'Borgia Clustering')):
    '''

    :param evaluation_tuples:
    :return:
    '''
    #Algorithm order: greedy_results, gn_results, flow_results, eigen_vector, info_map_vector, lovaina_results, b_results
    if len(evaluation_tuples[0]) == 2:
        supervised = True
        aux = []
        for tuple in evaluation_tuples:
            aux.append(tuple[0] + tuple[1])

        evaluation_tuples = aux
    else:
        supervised = False

    result = pd.DataFrame(np.array(evaluation_tuples))
    result.index = names

    if supervised:
        result.columns = eval_metrics
        result = result.reindex(columns=('Modularity', 'Modularity Density', 'ARI', 'NMI', 'Communities'))
        result.columns = ('Modularity', 'Modularity Density', 'ARI', 'NMI', 'Communities')
    else:
        result.columns = eval_metrics[0:3]

    return result

def measure_time(adjacency_list, control, N = 1, sr = 1):
    import time
    tiempos = [0]*N
    
    for i in range(N):
        array_affinity, array_connection, df_affinity, diccionario_masas, dic, rdic, link = full_process_csv(adjacency_list)
        masses = np.sum(array_connection,1).reshape((array_connection.shape[0],1))
        start = time.time()
        borgia_clustering(array_affinity, masses, control, dic, rdic, [1])
        end = time.time()
        tiempos[i] = (end - start)

    return tiempos

def full_time_measure(adjacency_list, control, N = 30):
    tiempos = np.arange(0.1, 1.1, 0.1)

    tiempos_df = pd.DataFrame(np.zeros((N, len(tiempos))))
    for i in range(len(tiempos)):
        ans = measure_time(random_sample(adjacency_list, tiempos[i]), control, N=N, sr=tiempos[i])
        tiempos_df.iloc[:,i] = ans

    tiempos_df.to_csv('result_big_dynamic_2.csv')

    return tiempos_df    

def calc_result_clustering(adjacency_list, file_name = 'borgia_result.pdf'):
    array_affinity, array_connection, df_affinity, diccionario_masas, dic, rdic, link = full_process_csv(adjacency_list, csr=control['sparse'], symmetric=control['sym'])
    masses = np.sum(array_connection, 1).reshape(array_connection.shape[0],1)
    import time
    start = time.time()
    clustering = borgia_clustering(array_affinity, masses, control, dic, rdic, [1])
    end = time.time()
    #return (end - start)
    try:
        plot_dendrogram(clustering, df_affinity.columns, file_name)
    except:
        print('Something happened and the dendrogram could not be written to a file.')
    return clustering, array_connection, dic, rdic, (end - start)

def zachary_experiment():
    adjacency_list, _ = load_zachary()
    control['sym'] = True
    clustering = calc_result_clustering(adjacency_list, file_name = 'borgia_zachary_result.pdf')[0]
    print(eval_zachary(clustering))

def get_partitions(G, clustering):
    '''

    :param G:
    :param clustering:
    :return:
    '''
    communities = np.unique(clustering)
    node_sets = []
    nodes = np.array(G.nodes)

    for c in communities:
        chosen_nodes = np.equal(clustering, np.array(c))
        node_sets.append([nodes[i] for i in range(len(nodes)) if chosen_nodes[i]])

    return node_sets

# =============================================================================
# ~CONTROL PARAMETERS
# =============================================================================
#Good default configuration. You can play with the alpha and the penalty parameter (Early Roman Escalability policy)
control={}
control['delta'] = 0.1
control['num_clusters'] = 0
control['h'] = 1
control['unitary'] = False
control['adaptativo'] = False
control['penalty'] = 3
control['p'] = 0
control['connect'] = True
control['normalization'] = False
control['cl_test'] = False
control['correcion_dim'] = False
control['verbose'] = False
control['sparse'] = False
control['sym'] = False
control['alpha'] = 0.7

if __name__ == '__main__':
    #np.seterr(all='raise')
    mode = -10
    import speech_to_adjacency as sa
    if mode == 10:
        print('Euro vision last decade')
        adjacency_list = load_euro_finals('J', [2007, 2009, 2010, 2011,  2012, 2013, 2014, 2015, 2016, 2017, 2017])
        ans1 = format_output_evals(eval_net_algs(adjacency_list))
    elif mode == 11:
        print('Euro vision all years')
        adjacency_list = load_euro_finals('J', None)
        ans2 = format_output_evals(eval_net_algs(adjacency_list))
    elif mode == -11:
        print('Euro vision all years')
        adjacency_list = load_euro_finals('J', None)
        clustering_result, array_connection, dic, rdic, _ = calc_result_clustering(adjacency_list, file_name='borgia_result_euro_full.pdf')
    elif mode == -10:
        print('Euro vision last decade')
        adjacency_list = load_euro_finals('J', [2007, 2009, 2010, 2011,  2012, 2013, 2014, 2015, 2016, 2017, 2017])
        clustering_result, array_connection, dic, rdic, _ = calc_result_clustering(adjacency_list, file_name='borgia_result_euro_decade.pdf')
    elif mode == 1:
        print('Zachary karate')
        adjacency_list, labels = load_zachary()
        zachary_experiment()
        ans = format_output_evals(eval_net_algs(adjacency_list, labels))

    elif mode == 2:
        print('Dolphins')
        dolphins, labels = load_dolphins()
        ans = format_output_evals(eval_net_algs(dolphins, labels))

    elif mode == 3:
        print('Football')
        football, labels = load_football()
        ans =  format_output_evals(eval_net_algs(football, labels))

    elif mode == 4:
        print('Political blogs')
        pol, labels = load_polbooks()
        ans = format_output_evals(eval_net_algs(pol, labels))

    elif mode == 5:
        print('Heart of Darkness')
        heart = load_heart_of_darkness()
        ans = format_output_evals(eval_net_algs(heart))
    elif mode == -5:
        print('Heart of Darkness')
        heart = load_heart_of_darkness()
        clustering_result, array_connection, dic, rdic, _ = calc_result_clustering(heart, file_name='borgia_result_heart.pdf')

    elif mode == 6:
        print('Game of Thrones')
        got = load_got_cut()
        ans = format_output_evals(eval_net_algs(got))
    elif mode == -6:
        print('Game of Thrones')
        got = load_got_cut()
        clustering_result, array_connection, dic, rdic, _ = calc_result_clustering(got, file_name='borgia_result_got.pdf')
        sa.clustering_to_csv(dic, clustering_result, './got_35_clust.csv')
        
    elif mode == 7:
        print('Checking deltas for best alpha parameter')
        adjacency_list = load_got_cut()
        # Borgia Algorithm
        res = pd.DataFrame(columns=['G', 'p', 'alpha', 'Modularity', 'Modularity density', '#C'])
        for i in np.arange(0,1.1, 0.1):
            print(i)
            control['alpha'] = i
            clustering_result, array_connection, dic, rdic, _ = calc_result_clustering(adjacency_list,
                                                                                       file_name='borgia_result.pdf')
            G = nx.Graph(array_connection)
            G = nx.relabel_nodes(G, dict((v, k) for k, v in dic.items()))
            G2 = numpy_array_to_igraph(array_connection)

            for ix, elem in dic.items():
                dic[ix] = clustering_result[elem]

            # bans = get_partitions(G, clustering_result)
            b_results = compute_eval_metrics(G, array_connection, clustering_result, None)
            add = {'G': '1', 'p': control['penalty'], 'alpha':i, 'Modularity':b_results[0], 'Modularity density':b_results[1], '#C': b_results[2]}
            res = res.append(add, ignore_index=True)
    elif mode == 8:
        print('Checking deltas for best p parameter')
        adjacency_list = load_euro_finals('J', [2007, 2009, 2010, 2011,  2012, 2013, 2014, 2015, 2016, 2017, 2017])
        # Borgia Algorithm
        res = pd.DataFrame(columns=['G', 'p', 'alpha', 'Modularity', 'Modularity density', '#C'])
        for i in np.arange(0, 10):
            print(i)
            control['penalty'] = i
            clustering_result, array_connection, dic, rdic, _ = calc_result_clustering(adjacency_list,
                                                                                       file_name='borgia_result.pdf')
            G = nx.Graph(array_connection)
            G = nx.relabel_nodes(G, dict((v, k) for k, v in dic.items()))
            G2 = numpy_array_to_igraph(array_connection)

            for ix, elem in dic.items():
                dic[ix] = clustering_result[elem]

            # bans = get_partitions(G, clustering_result)
            b_results = compute_eval_metrics(G, array_connection, clustering_result, None)
            add = {'G': '1', 'p': control['penalty'], 'alpha': control['alpha'], 'Modularity': b_results[0],
                   'Modularity density': b_results[1], '#C': b_results[2]}
            res = res.append(add, ignore_index=True)

    elif mode == 9:
        print('Checking deltas for best Overlap')
        adjacency_list = load_euro_finals('J', [2007, 2009, 2010, 2011,  2012, 2013, 2014, 2015, 2016, 2017, 2017])
        # Borgia Algorithm
        res = pd.DataFrame(columns=['G', 'p', 'alpha', 'Modularity', 'Modularity density', '#C'])
        for i in np.arange(0, 10):
            print(i)
            control['p'] = i
            clustering_result, array_connection, dic, rdic, _ = calc_result_clustering(adjacency_list,
                                                                                       file_name='borgia_result.pdf')
            G = nx.Graph(array_connection)
            G = nx.relabel_nodes(G, dict((v, k) for k, v in dic.items()))
            G2 = numpy_array_to_igraph(array_connection)

            for ix, elem in dic.items():
                dic[ix] = clustering_result[elem]

            # bans = get_partitions(G, clustering_result)
            b_results = compute_eval_metrics(G, array_connection, clustering_result, None)
            add = {'G': '(xy)^' + str(control['p']), 'p': control['penalty'], 'alpha': control['alpha'], 'Modularity': b_results[0],
                   'Modularity density': b_results[1], '#C': b_results[2]}
            res = res.append(add, ignore_index=True)

    
   